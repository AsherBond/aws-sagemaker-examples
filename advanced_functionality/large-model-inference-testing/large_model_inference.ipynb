{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Model Inference Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "This notebook is a **blueprint** for real-time [large model inference (LMI)](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference.html) testing in SageMaker. This notebook uses [Deep Java Library (djl) Large Model Inference (LMI) engines](https://docs.djl.ai/docs/serving/serving/docs/lmi/conceptual_guide/lmi_engine.html) to deploy the models in SageMaker.\n",
    "\n",
    "This notebook is driven by [LMI configuration YAML files](#lmi-configuration-file). The `test` object in LMI configuration file defines the testing module interface. The testing module provides a prompt generator class that yields prompts for synchronous request-response tests. The prompt used in each test, the generated text, the request latency (secs), the number of output tokens, and tokens per second are recorded in a multi-line JSON output file. \n",
    "\n",
    "This notebook does not run concurrent requests to measure request throughput. If such a test is needed, the SageMaker endpoint created by this notebook can be used with your benchmark client.\n",
    "\n",
    "When comparing latency across different instance types, keep in mind that the LMI engines maybe different. Ensure consistency in LMI engine configurations, as best as possible, so that latency comparison is meaningful. Inconsistencies among LMI configurations can render comparisons across instance types largely meaningless. Different LMI engines will produce different results for the same model and prompt. Therefore, in addition to comparing latency, compare tokens-per-second.\n",
    "\n",
    "## Hardware Requirements for Running Notebook\n",
    "\n",
    "Since we are working with large model inference, we will need to download HuggingFace model snapshots, and upload them to S3 bucket. Configure at least 1000 GB volume with the SageMaker Notebook instance. For SageMaker notebook instance type, `ml.m5.2xlarge`, or larger, is recommended..\n",
    "\n",
    "## Manually Validate Model Outputs\n",
    "\n",
    "After deploying a new model, do a trial run and manually validate that generated text is not gibberish. If the model is producing gibberish, try deleting and redeploying the model.\n",
    "\n",
    "## LMI Configuration File\n",
    "\n",
    "To deploy a model, you need to define a LMI configuration (YAML) file. Below, we show an example LMI configuration file:\n",
    "\n",
    "```\n",
    "huggingface:\n",
    "  model: \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "  revision: \"41b61a33a2483885c981aa79e0df6b32407ed873\"\n",
    "  download: true\n",
    "djl:\n",
    "  engine: \"MPI\"\n",
    "  option.entryPoint: \"djl_python.deepspeed\"\n",
    "  option.tensor_parallel_degree: 4\n",
    "  option.model_loading_timeout: 1800\n",
    "  option.dtype: \"fp16\"\n",
    "  option.max_tokens: 2048\n",
    "  option.task: \"text-generation\"\n",
    "sagemaker:\n",
    "  model:\n",
    "    name: \"mistral-7b-instruct-v0-2-deepspeed\"\n",
    "    container: \"containers/deepspeed\"\n",
    "    env: \n",
    "      HUGGINGFACE_HUB_CACHE: \"/tmp\"\n",
    "      TRANSFORMERS_CACHE: \"/tmp\"\n",
    "  endpoint:\n",
    "    name: \"mistral-7b-instruct-v0-2-deepspeed\"\n",
    "    instance_type: \"ml.g5.12xlarge\"\n",
    "    initial_instance_count: 1\n",
    "    variant_name: \"test\"\n",
    "    model_data_download_timeout_secs: 1800\n",
    "    container_startup_health_check_timeout_secs: 1200\n",
    "test:\n",
    "  module_name: \"prompt_generator\"\n",
    "  module_dir: \"modules/inst-semeval2017\"\n",
    "  prompt_generator: \"PromptGenerator\"\n",
    "  params: { \"do_sample\": true, \"max_new_tokens\": 1024, \"top_k\": 50 }\n",
    "  warmup_iters: 1\n",
    "  max_iters: 10\n",
    "  output_dir: \"output/mistral-7b-instruct-v0.2/deepspeed\"\n",
    "```\n",
    "\n",
    "Below, we provide a brief explanation for the LMI configuration file:\n",
    "\n",
    "* The `huggingface` object is optional: Alternatively, `djl.option\\.model_id` must be specified. \n",
    "    * If you specify `huggingface` object, the `huggingface.name` field is required\n",
    "    * The `huggingface.revision` is required if `huggingface.download` is `true`.\n",
    "* The `djl` object contains the content for [DJL serving.properties](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-configuration.html) for the selected [LMI engine](https://docs.djl.ai/docs/serving/serving/docs/lmi/user_guides/index.html)\n",
    "    * If you specify `huggingface` object, `djl.option\\.model_id` is computed automatically.\n",
    "* The `sagemaker.model` object is required \n",
    "    * The field `sagemaker.model.image` is optional: Alternatively, you can specify `sagemaker.model.container`  as relative path (w.r.t. to this notebook) to the container build script directory.\n",
    "    * The field `sagemaker.model.env` is optional\n",
    "* The `sagemaker.endpoint` object is required\n",
    "    * In `sagemaker.endpoint` object, only `name` and `instance_type` are required.\n",
    "* The `test` object is optional, and defines the test interface \n",
    "    * The `test.module_dir` is relative path. This path is added to `sys.path`. If there is a `requirements.txt` in this directory, it is installed.\n",
    "    * The `test.module_name` must be a Python module in `test.module_dir`. This module is dynamically loaded.\n",
    "    * The `test.prompt_generator` is the name of a class in the `test.module_name` module. An object of this class is dyamically created, and the `__call__` method on the object is called to get the prompt generator for testing.\n",
    "    * The `test.warmpup_iters` are used to warmup the deployed inference model.\n",
    "    * The `test.max_iters` limits the number of prompt requests, not including the `test.warmup_iters`.\n",
    "    * The `test.output_dir` is the relative path where `results.json` testing output file is written. \n",
    "    * Each line in `results-*.json` file is a json object with following fields: request `prompt`, request output `text`, and request `latency` in seconds.\n",
    "### Custom Model Handler Code\n",
    "\n",
    "You can optionally add a [custom model handler](https://github.com/deepjavalibrary/djl-serving/blob/bc7fdfdcbb66b982522e6bc809b0044fabde69e0/serving/docs/streaming_config.md#custom-modelpy-handler) in the `code` sub-folder, collocated with the LMI configuration file, and it is added to the SageMaker model package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize SageMaker Session\n",
    "\n",
    "Let us specify the `s3_bucket` and `s3_prefix` that we will use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "s3_bucket  =  None # specify bucket, or use default sagemaker bucket, if it exists\n",
    "s3_prefix = 'lmi-djl' # Large model inference with deep java library\n",
    "\n",
    "role = get_execution_role() # you may provide a pre-existing role ARN here\n",
    "print(f\"SageMaker Execution Role: {role}\")\n",
    "\n",
    "session = boto3.session.Session()\n",
    "aws_region = session.region_name\n",
    "print(f\"AWS Region: {aws_region}\")\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(boto_session=session)\n",
    "\n",
    "try:\n",
    "    if s3_bucket is None:\n",
    "        s3_bucket = sagemaker_session.default_bucket()\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.get_bucket_location(Bucket=s3_bucket)\n",
    "    bucket_region = response['LocationConstraint']\n",
    "    bucket_region = 'us-east-1' if bucket_region is None else bucket_region\n",
    "    \n",
    "    print(f\"Bucket region: {bucket_region}\")\n",
    "    \n",
    "    try:\n",
    "        s3_client.head_object(Bucket=s3_bucket, Key=f\"{s3_prefix}/\")\n",
    "    except:\n",
    "        s3_client.put_object(Bucket=s3_bucket, Key=f\"{s3_prefix}/\")\n",
    "\n",
    "    print(f\"Using S3 folder: s3://{s3_bucket}/{s3_prefix}/ in this notebook\")\n",
    "except:\n",
    "    print(f\"Access Error: Check if '{s3_bucket}' S3 bucket is in '{aws_region}' region, and {s3_prefix} path exists\")\n",
    "\n",
    "sts = boto3.client(\"sts\")\n",
    "aws_account_id = sts.get_caller_identity()[\"Account\"]\n",
    "\n",
    "print(f\"AWS Account Id: {aws_account_id}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify SageMaker LMI configuration\n",
    "\n",
    "We specify a *SageMaker LMI configuration* YAML file for the model we wish to deploy. For example, we specify [DeepSpeed LMI configuration file for mistral-7b-instruct-v0.2](./examples/deepspeed/mistral-7b-instruct-v0.2/config.yaml), below. You can specify whatever example file you wish to deploy.\n",
    "\n",
    "**Tip:** \n",
    "You may wish to make copies of this notebook, if you would like to work with multiple examples concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "\n",
    "import pathlib\n",
    "print(f\"Current working directory: {pathlib.Path().resolve()}\")\n",
    "\n",
    "config_path=\"examples/transformers-neuronx/llama2-7b-chat/config-streaming.yaml\"\n",
    "with open(config_path, \"r\") as mf:\n",
    "    model_config=yaml.safe_load(mf)\n",
    "\n",
    "print(\"\\nmodel_config:\\n\")\n",
    "print(json.dumps(model_config, indent=2))\n",
    "\n",
    "assert model_config.get('djl', None), \"'djl' object is required\"\n",
    "\n",
    "assert model_config.get('huggingface', None) or model_config['djl'].get('option.model_id', None), \\\n",
    "    \"'huggingface'  object or 'djl.option\\.model_id' is required\"\n",
    "\n",
    "assert model_config.get('sagemaker', None), \"'sagemaker' object is required\"\n",
    "assert model_config['sagemaker'].get('model',None), \"'sagemaker.model' is required\"\n",
    "assert model_config['sagemaker'].get('endpoint', None), \"'sagemaker.endpoint' is required\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe Build and Push Model Inference Container Image to ECR\n",
    "\n",
    "Next, if `sagemaker.model.container` is specified, we build and push the container image to Amazon ECR. After building and pushing the image to ECR, we update the `sagemaker.model.image` with the ECR URI for the image. Either `container` or `image` must be specified in `sagemaker.model`.\n",
    "\n",
    "**Tip:** After building and pushing the container image to ECR, you may want to update `sagemaker.model.image` field in the LMI configuration file with the ECR URI, and delete the image from your local docker repository. This will free up space to build additional docker images, or you are likely to run of of local disk space. Docker, by default, uses the root partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os, subprocess, stat\n",
    "\n",
    "sm_model_config = model_config['sagemaker']['model']\n",
    "container_path = sm_model_config.get(\"container\", None)\n",
    "\n",
    "if container_path is not None:\n",
    "    with open(os.path.join(container_path, \"build.log\"), \"w\") as logfile:\n",
    "            print(f\"Building and pushing {container_path} to ECR; see log file: {container_path}/build.log\")\n",
    "            container_build_script = os.path.join(container_path, \"build_tools\", \"build_and_push.sh\")\n",
    "\n",
    "            st = os.stat(container_build_script)\n",
    "            os.chmod(container_build_script, st.st_mode | stat.S_IEXEC)\n",
    "            subprocess.check_call([container_build_script, aws_region], stdout=logfile, stderr=subprocess.STDOUT)\n",
    "\n",
    "            image_tag = !cat {container_path}/build_tools/set_env.sh \\\n",
    "                | grep 'IMAGE_TAG' | sed 's/.*IMAGE_TAG=\\(.*\\)/\\1/'\n",
    "\n",
    "            image_name = !cat {container_path}/build_tools/set_env.sh \\\n",
    "                | grep 'IMAGE_NAME' | sed 's/.*IMAGE_NAME=\\(.*\\)/\\1/'\n",
    "\n",
    "            ecr_image_uri=f\"{aws_account_id}.dkr.ecr.{aws_region}.amazonaws.com/{image_name[0]}:{image_tag[0]}\"\n",
    "\n",
    "    sm_model_config['image'] = ecr_image_uri\n",
    "else:\n",
    "    assert sm_model_config.get('image', None) is not None, \"'sagemaker.model.image' or 'sagemaker.model.container' is required\"\n",
    "\n",
    "print(f\"Model serving image: {sm_model_config['image']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe Download HuggingFace Model Snapshot\n",
    "\n",
    "If `huggingface` object is specified, only `huggingface.model` is required. If `huggingface.download` is `true`, the HuggingFace model snapshot with revision `huggingface.revision` is downloaded and uploaded to the `s3_bucket`, and `djl.option\\.model_id` is set to the S3 URI of the uploaded HuggingFace model snapshot. \n",
    "\n",
    "**Note: You must specify `token` below if `huggingface.download`  is `true`, and the `huggingface.model` requires a HuggingFace token for downloading from the HuggingFace hub.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hf_spec = model_config.get('huggingface', None)\n",
    "\n",
    "if hf_spec:\n",
    "    hf_model = hf_spec.get('model', None)\n",
    "    download = hf_spec.get('download', None)\n",
    "    if download:\n",
    "        revision = hf_spec.get('revision', None)\n",
    "        assert revision, \"'huggingface.revision' is required if 'download' is 'true'\"\n",
    "        \n",
    "        token = None # Specify HuggingFace token, if required by model\n",
    "\n",
    "        s3_model_prefix = f\"{s3_prefix}/huggingface/models/{hf_model}/{revision}\"  # folder where model checkpoint will go\n",
    "        print(f\"s3_model_prefix: {s3_model_prefix}\")\n",
    "\n",
    "        try:\n",
    "            s3_client.head_object(Bucket=s3_bucket, Key=f\"{s3_model_prefix}/config.json\")\n",
    "            print(f\"Skipping download; HuggingFace model already exists at s3://{s3_bucket}/{s3_model_prefix}/\")\n",
    "        except:\n",
    "            subprocess.check_output(f\"pip install huggingface-hub\", shell=True, stderr=subprocess.STDOUT)\n",
    "            from huggingface_hub import snapshot_download\n",
    "            from tempfile import TemporaryDirectory\n",
    "            from pathlib import Path\n",
    "\n",
    "            print(f\"Downloading HuggingFace model snapshot: {hf_model}, revision: {revision}\")\n",
    "            with TemporaryDirectory(suffix=\"model\", prefix=\"hf\", dir=\".\") as cache_dir:\n",
    "                ignore_patterns = [\"*.msgpack\", \"*.h5\"]\n",
    "                snapshot_download(repo_id=hf_model, \n",
    "                    revision=revision, \n",
    "                    cache_dir=cache_dir,\n",
    "                    ignore_patterns=ignore_patterns,\n",
    "                    token=token)\n",
    "\n",
    "                local_model_path = Path(cache_dir)\n",
    "                model_snapshot_path = str(list(local_model_path.glob(f\"**/snapshots/{revision}\"))[0])\n",
    "                print(f\"model_snapshot_path: {model_snapshot_path}\")\n",
    "\n",
    "                for root, dirs, files in os.walk(model_snapshot_path):\n",
    "                    for file in files:\n",
    "                        full_path = os.path.join(root, file)\n",
    "                        with open(full_path, 'rb') as data:\n",
    "                            key = f\"{s3_model_prefix}/{full_path[len(model_snapshot_path)+1:]}\"\n",
    "                            s3_client.upload_fileobj(data, s3_bucket, key)\n",
    "\n",
    "        model_s3_url = f\"s3://{s3_bucket}/{s3_model_prefix}/\"\n",
    "        model_config['djl']['option.model_id'] = model_s3_url\n",
    "    else:\n",
    "        model_config['djl']['option.model_id'] = hf_model\n",
    "    \n",
    "    print(json.dumps(model_config, indent=2))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Package\n",
    "\n",
    "Next, we create the model package TAR ball, and upload it to `s3_bucket`. The model package will be used to define the SageMaker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory, NamedTemporaryFile\n",
    "from pathlib import Path\n",
    "import glob \n",
    "import shutil\n",
    "import os\n",
    "import tarfile\n",
    "\n",
    "sm_model_name = sm_model_config.get('name', None)\n",
    "assert sm_model_name, \"'sagemaker.model.name' is required\"\n",
    "\n",
    "with TemporaryDirectory(suffix=\"pkg\", prefix=\"model\", dir=\".\") as pkg_dir:\n",
    "\n",
    "    with open(os.path.join(pkg_dir, \"serving.properties\"), \"w\") as props_file:\n",
    "        for key, value in model_config['djl'].items():\n",
    "            props_file.write(f\"{key}={value}\\n\")\n",
    "\n",
    "    code_dir = os.path.join(os.path.dirname(config_path), \"code\")\n",
    "    if os.path.isdir(code_dir):\n",
    "        files = glob.glob(f\"{code_dir}/*\")\n",
    "\n",
    "        for file in files:\n",
    "            if os.path.isdir(file):\n",
    "                shutil.copytree(file, os.path.join(pkg_dir, os.path.basename(file)))\n",
    "            else:\n",
    "                shutil.copy2(file, pkg_dir)\n",
    "\n",
    "    with NamedTemporaryFile(prefix=\"model\", suffix=\".gz\") as gz_file:\n",
    "        with tarfile.open(gz_file.name, \"w:gz\") as tar:\n",
    "            tar.add(pkg_dir, arcname=\"\")\n",
    "\n",
    "        gz_file.seek(0)\n",
    "        model_pkg_key = f\"{s3_prefix}/sagemaker/code/{sm_model_name}/model.tar.gz\"\n",
    "        print(f\"Upload model package to s3://{s3_bucket}/{model_pkg_key}\")\n",
    "        s3_client.upload_fileobj(gz_file, s3_bucket, model_pkg_key)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Model\n",
    "\n",
    "Next, we create the SageMaker model using the model package we just uploaded tp `s3_bucket`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "sm_model_image = sm_model_config.get(\"image\", None)\n",
    "assert sm_model_image, \"'sagemaker.model.image' is required\"\n",
    "\n",
    "sm_model_env = sm_model_config.get(\"env\", {})\n",
    "\n",
    "try:\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName=sm_model_name,\n",
    "        ExecutionRoleArn=role,\n",
    "        PrimaryContainer={\n",
    "            \"Image\": sm_model_image,\n",
    "            \"ModelDataUrl\": f\"s3://{s3_bucket}/{model_pkg_key}\",\n",
    "            \"Environment\": sm_model_env,\n",
    "        },\n",
    "    )\n",
    "    model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "    print(f\"Created Model: {model_arn}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint Config\n",
    "\n",
    "Next we create endpoint config for the SageMaker model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_endpoint_spec = model_config['sagemaker']['endpoint']\n",
    "endpoint_name = sm_endpoint_spec.get('name', None)\n",
    "assert endpoint_name, \"'sagemaker.endpoint.name' is required\"\n",
    "\n",
    "variant_name = sm_endpoint_spec.get('variant_name', 'test')\n",
    "\n",
    "instance_type=sm_endpoint_spec.get('instance_type', None)\n",
    "assert instance_type, \"'sagemaker.endpoint.instance_type' is required\"\n",
    "\n",
    "initial_instance_count=sm_endpoint_spec.get('initial_instance_count', 1)\n",
    "model_data_download_timeout_secs = sm_endpoint_spec.get('model_data_download_timeout_secs', 1200)\n",
    "container_startup_health_check_timeout_secs=sm_endpoint_spec.get('container_startup_health_check_timeout_secs', 1200)\n",
    "\n",
    "production_variant = {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"ModelName\": sm_model_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": initial_instance_count,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_secs,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_secs,\n",
    "        }\n",
    "\n",
    "volume_size_gb = sm_endpoint_spec.get('volume_size_gb', None)\n",
    "if volume_size_gb:\n",
    "    production_variant['VolumeSizeInGB'] = volume_size_gb\n",
    "\n",
    "try:\n",
    "    endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_name,\n",
    "        ProductionVariants=[production_variant]\n",
    "    )\n",
    "    print(endpoint_config_response)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating endpoint config: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Endpoint\n",
    "\n",
    "Next, we create the SageMaker Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    create_endpoint_response = sm_client.create_endpoint(EndpointName=f\"{endpoint_name}\", \n",
    "                                                     EndpointConfigName=endpoint_name)\n",
    "    print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating endpoint: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Testing\n",
    "\n",
    "Now, we are ready to run our tests, using the test interface. During this step, we install HuggingFace  `transformers` library, download model tokenizer configuration files, and create a tokenizer. Next, we load the test interface Python module, create a prompt generator class object, and use it to drive our test run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from importlib import import_module\n",
    "import sys\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "from generate import generate\n",
    "from utils import get_tokenizer\n",
    "\n",
    "sm_runtime_client = boto3.client(\"runtime.sagemaker\")\n",
    "\n",
    "test_spec = model_config.get('test', None)\n",
    "if test_spec:\n",
    "    print(\"installing transformers package\")\n",
    "    subprocess.check_output(f\"pip install transformers\", shell=True, stderr=subprocess.STDOUT)\n",
    "    \n",
    "    module_name = test_spec.get('module_name', None)\n",
    "    assert module_name, \"'test.module_name' is required\"\n",
    "    \n",
    "    module_dir = test_spec.get('module_dir', None)\n",
    "    assert module_name, \"'test.module_dir' is required\"\n",
    "    \n",
    "    prompt_generator = test_spec.get('prompt_generator', None)\n",
    "    assert prompt_generator, \"'test.prompt_generator' is required\"\n",
    "    \n",
    "    output_dir = test_spec.get('output_dir', None)\n",
    "    assert output_dir, \"'test.output_dir' is required\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    sys.path.append(module_dir)\n",
    "    \n",
    "    requirements_path = os.path.join(module_dir, \"requirements.txt\")\n",
    "    if os.path.isfile(requirements_path):\n",
    "        print(f\"Installing test module requirements...\")\n",
    "        subprocess.check_output(f\"pip install -r {requirements_path}\", shell=True, stderr=subprocess.STDOUT)\n",
    "    \n",
    "    print(f\"Loading test module: {module_name} from {module_dir}\")\n",
    "    mod=import_module(module_name)\n",
    "    prompt_generator_class = getattr(mod, prompt_generator)\n",
    "    \n",
    "    print(f\"Creating prompt generator object for class: {prompt_generator_class}\")\n",
    "    prompt_generator = prompt_generator_class()()\n",
    "\n",
    "    warmup_iters = int(test_spec.get('warmup_iters', 1))\n",
    "    max_iters = int(test_spec.get('max_iters', 10))\n",
    "    params = test_spec.get(\"params\", None)\n",
    "    \n",
    "    ts = strftime(\"%Y-%m-%d-%H-%M-%S-GMT\", gmtime())\n",
    "    cumu_time = 0.0\n",
    "    cumu_tokens = 0\n",
    "    cumu_ttft = 0.0\n",
    "    \n",
    "    tokenizer = get_tokenizer(s3_client, model_config['djl']['option.model_id'])\n",
    "    \n",
    "    output_formatter = model_config['djl'].get(\"option.output_formatter\", None)\n",
    "    rolling_batch = model_config['djl'].get(\"option.rolling_batch\", None)\n",
    "    streaming_enabled = rolling_batch in [ \"auto\", \"deepspeed\", \"trtllm\" ] and output_formatter in [ \"jsonlines\"]\n",
    "                                            \n",
    "    print(f\"Streaming enabled: {streaming_enabled}\")\n",
    "    \n",
    "    try:\n",
    "        results_path = os.path.join(output_dir, f\"results-{instance_type}-{ts}.json\")\n",
    "        with open(results_path, \"w\") as results:\n",
    "            count = 0\n",
    "            \n",
    "            print(\"Start testing...\")\n",
    "            while prompt := next(prompt_generator):\n",
    "                ttft = None\n",
    "                start_time = time.time()\n",
    "                \n",
    "                text, ttft = generate(sm_runtime_client, endpoint_name, prompt, params=params, stream=streaming_enabled)\n",
    "                latency = time.time() - start_time\n",
    "                    \n",
    "                count += 1\n",
    "                if count <= warmup_iters:\n",
    "                    print(f\"Warm up iteration: {count} of {warmup_iters}. latency: {latency}, ttft: {ttft}\")\n",
    "                    continue\n",
    "                \n",
    "                if ttft:\n",
    "                    cumu_ttft += ttft\n",
    "                \n",
    "                iter_count = count - warmup_iters\n",
    "                \n",
    "                cumu_time += latency\n",
    "                index = text.find(prompt)\n",
    "                if index != -1:\n",
    "                    text = text[len(prompt):]\n",
    "                    \n",
    "                n_tokens = len(tokenizer.encode(text))\n",
    "                cumu_tokens += n_tokens\n",
    "                \n",
    "                tps = n_tokens/latency\n",
    "                \n",
    "                json_obj = {\"prompt\": prompt, \n",
    "                            \"text\": text, \n",
    "                            \"n_tokens\": n_tokens,\n",
    "                            \"latency\": latency, \n",
    "                            \"tps\": tps,\n",
    "                            \"ttft\": ttft}\n",
    "                \n",
    "                results.write(json.dumps( json_obj )+\"\\n\")   \n",
    "                avg_latency = cumu_time/iter_count\n",
    "                avg_tps = cumu_tokens/cumu_time\n",
    "                avg_tokens = cumu_tokens/iter_count\n",
    "                avg_ttft = cumu_ttft/iter_count\n",
    "                \n",
    "                print(f\"Iterations completed: {iter_count} of {max_iters}; avg_tokens: {avg_tokens}, avg_latency: {avg_latency} secs, avg_tps: {avg_tps}, avg_ttft: {avg_ttft}\")\n",
    "                if iter_count >= max_iters:\n",
    "                    break    \n",
    "        print(f\"Testing completed. Results file: {results_path}\")\n",
    "    except StopIteration:\n",
    "        pass\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "This concludes the notebook. Below. we delete the **deployed** SageMaker endpoint, endpoint configuration, and the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "print(f\"Delete Endpoint response: {response}\")\n",
    "\n",
    "response = sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)\n",
    "print(f\"Delete Endpoint Config response: {response}\")\n",
    "\n",
    "response = sm_client.delete_model(ModelName=sm_model_name)\n",
    "print(f\"Delete Model response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/advanced_functionality|distributed_tensorflow_mask_rcnn|mask-rcnn-scriptmode-fsx.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
