{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a05c0d-3a13-4c3d-924b-204722a3eb74",
   "metadata": {},
   "source": [
    "# Large Model Inference Testing Analysis\n",
    "\n",
    "This notebook helps us anlayze the results of the large model inference testing. We start by first speciying the `model` below for which we want to analyze the results. We list all the folders under the specified `model` folder to analyze the output results: Each folder corresponds to to a [Deep Java Library LMI engine](https://docs.djl.ai/docs/serving/serving/docs/lmi/conceptual_guide/lmi_engine.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540d5b7-79c4-4426-a002-56f773dbd3f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "model=\"llama-2-7b-chat-hf\"\n",
    "assert model is not None, \"Please specify model\"\n",
    "\n",
    "directory = os.path.join(Path().resolve(), \"output\", f'{model}')\n",
    "\n",
    "path = Path(directory)\n",
    "\n",
    "lmi_dirs = [p for p in path.iterdir() if p.is_dir()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a97793-d1ef-4e77-b663-3c3406dc25f6",
   "metadata": {},
   "source": [
    "## Find Latest Results\n",
    "\n",
    "For each [LMI engine](https://docs.djl.ai/docs/serving/serving/docs/lmi/conceptual_guide/lmi_engine.html), we scan the results for the specified model, and select the latest results, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382fe0d3-75b0-479a-a26a-2edf5dc4bf77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "results_latest = []\n",
    "for lmi_dir in lmi_dirs:\n",
    "    all_files = glob.glob(os.path.join(lmi_dir, \"results-*.json\"))\n",
    "    results_latest.append(max(all_files, key=os.path.getmtime))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce610c7-023f-41bb-b444-d37544d3be2f",
   "metadata": {},
   "source": [
    "## Read the Results into Pandas DataFrames\n",
    "\n",
    "We read the latest results for the specified `model` into Pandas data frames, using one data frame for each [Deep Java Library LMI engine](https://docs.djl.ai/docs/serving/serving/docs/lmi/conceptual_guide/lmi_engine.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67acdee7-a81d-4b7d-b033-007b74659ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdfs = []\n",
    "for result in results_latest:\n",
    "    pdfs.append(pd.read_json(result, lines=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29c3b6-e5a2-4e39-b742-5d481e5205d2",
   "metadata": {},
   "source": [
    "## Display Top Rows from Each Data Frame\n",
    "\n",
    "For each Pandas data frame, we display `top_n` rows. The columns in each data frame are as follows:\n",
    "\n",
    "1. `prompt` column contains the prompt\n",
    "2. `text` column contains the complete generated text, which includes the `prompt`\n",
    "3. `n_tokens` column contains number of tokens in text\n",
    "4. `latency` column contains the total request latency from sending the request to receiving the complete response\n",
    "5. `tps` column contains tokens per second, which is just number of tokens in the request divided by the request latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c179c99-884e-4203-8ff7-8f824a0719c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "top_n = 2\n",
    "for i, df in enumerate(pdfs):\n",
    "    caption=f\"{model}/{os.path.basename(lmi_dirs[i])}\".upper()\n",
    "    df = df.truncate(after=top_n - 1, axis=0)\n",
    "    df.index += 1\n",
    "    df = df.style \\\n",
    "      .format(precision=5) \\\n",
    "      .format_index(str.upper, axis=1) \\\n",
    "        .set_properties(**{'text-align': 'left'}) \\\n",
    "        .set_caption(caption)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df23639a-de07-488f-97f8-2c2e65c16cf6",
   "metadata": {},
   "source": [
    "## Results metrics\n",
    "\n",
    "Below we show the key results metrics, which include `n_tokens`, `latency`, and `tps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cea212-717b-4f59-adbe-87adb36c265f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "top_n = 2\n",
    "for i, df in enumerate(pdfs):\n",
    "    caption=f\"{model}/{os.path.basename(lmi_dirs[i])}\".upper()\n",
    "    df = df[['n_tokens', 'latency', 'tps', 'ttft']]\n",
    "    df.index += 1\n",
    "    df = df.style \\\n",
    "      .format(precision=5) \\\n",
    "      .format_index(str.upper, axis=1) \\\n",
    "        .set_properties(**{'text-align': 'left'}) \\\n",
    "        .set_caption(caption)\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfed60c-47b5-4173-848e-18654fc47e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
